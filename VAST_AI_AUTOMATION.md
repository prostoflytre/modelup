# Vast.ai –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è Fine-tuning LLM

–ü–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∞—Ä–µ–Ω–¥—ã GPU —Å–µ—Ä–≤–µ—Ä–æ–≤ –Ω–∞ Vast.ai, –¥–æ–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å LoRA –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

- [–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏](#–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏)
- [–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è](#—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è)
- [–£—Å—Ç–∞–Ω–æ–≤–∫–∞](#—É—Å—Ç–∞–Ω–æ–≤–∫–∞)
- [–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è](#–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è)
- [–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ](#–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ)
- [–ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç](#–∫–∞–∫-—ç—Ç–æ-—Ä–∞–±–æ—Ç–∞–µ—Ç)
- [–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è](#–ø–∞—Ä–∞–º–µ—Ç—Ä—ã-–æ–±—É—á–µ–Ω–∏—è)
- [–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞](#—Å—Ç—Ä—É–∫—Ç—É—Ä–∞-–ø—Ä–æ–µ–∫—Ç–∞)
- [Troubleshooting](#troubleshooting)

## üöÄ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫** —Å–∞–º—ã—Ö –¥–µ—à–µ–≤—ã—Ö GPU –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–∞ Vast.ai
- ‚úÖ **–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ü–µ–Ω–µ** - —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–π—Ç–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é —Å—Ç–æ–∏–º–æ—Å—Ç—å –≤ —á–∞—Å
- ‚úÖ **Fallback —Å–∏—Å—Ç–µ–º–∞** - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ—Ö–æ–¥ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –æ—Ñ—Ñ–µ—Ä—É –ø—Ä–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
- ‚úÖ **SSH –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ** —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
- ‚úÖ **Fine-tuning —Å LoRA** - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π
- ‚úÖ **4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è** —á–µ—Ä–µ–∑ BitsAndBytes –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM
- ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞** –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä
- ‚úÖ **–ê–≤—Ç–æ–æ—á–∏—Å—Ç–∫–∞** - —É–¥–∞–ª–µ–Ω–∏–µ –∏–Ω—Å—Ç–∞–Ω—Å–∞ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã

## üì¶ –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

### –õ–æ–∫–∞–ª—å–Ω–∞—è –º–∞—à–∏–Ω–∞
- Python 3.10+
- SSH –∫–ª–∏–µ–Ω—Ç (–≤—Å—Ç—Ä–æ–µ–Ω –≤ Windows 10+/Linux/macOS)
- –î–æ—Å—Ç—É–ø –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É

### Vast.ai –∞–∫–∫–∞—É–Ω—Ç
- –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–∫–∫–∞—É–Ω—Ç –Ω–∞ [vast.ai](https://vast.ai)
- API —Ç–æ–∫–µ–Ω (–ø–æ–ª—É—á–∞–µ—Ç—Å—è –≤ [Account Settings](https://cloud.vast.ai/account/))
- SSH –∫–ª—é—á –¥–æ–±–∞–≤–ª–µ–Ω –≤ –∞–∫–∫–∞—É–Ω—Ç ([SSH Keys Settings](https://cloud.vast.ai/account/ssh-keys/))

### Python –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
```bash
pip install requests
```

## üîß –£—Å—Ç–∞–Ω–æ–≤–∫–∞

1. **–ö–ª–æ–Ω–∏—Ä—É–π—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π**
```bash
git clone <your-repo>
cd modelup
```

2. **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏**
```bash
pip install requests
```

3. **–ù–∞—Å—Ç—Ä–æ–π—Ç–µ SSH –∫–ª—é—á**
   - –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ SSH –∫–ª—é—á (–µ—Å–ª–∏ –Ω–µ—Ç):
     ```bash
     ssh-keygen -t ed25519 -C "your_email@example.com"
     ```
   - –°–∫–æ–ø–∏—Ä—É–π—Ç–µ –ø—É–±–ª–∏—á–Ω—ã–π –∫–ª—é—á:
     ```bash
     cat ~/.ssh/id_ed25519.pub  # Linux/macOS
     type %USERPROFILE%\.ssh\id_ed25519.pub  # Windows
     ```
   - –î–æ–±–∞–≤—å—Ç–µ –≤ [Vast.ai SSH Keys](https://cloud.vast.ai/account/ssh-keys/)

4. **–ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è**
   - –§–æ—Ä–º–∞—Ç JSONL: –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ - JSON –æ–±—ä–µ–∫—Ç —Å –ø–æ–ª–µ–º `"text"`
   - –ü—Ä–∏–º–µ—Ä: `data/sample_training_data.jsonl`

## ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

–û—Ç–∫—Ä–æ–π—Ç–µ `vast.ai.check.py` –∏ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:

### API —Ç–æ–∫–µ–Ω Vast.ai
```python
BEARER_TOKEN = "–≤–∞—à_—Ç–æ–∫–µ–Ω_–∑–¥–µ—Å—å"
```

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ GPU
```python
gpu_list = ["RTX 4090", "RTX 5090", "Q RTX 8000", "RTX 6000Ada", "A4000"]
max_price = 0.4  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –≤ USD/—á–∞—Å
```

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
```python
# –í —Ñ–∞–π–ª–µ remote_train.py
MODEL_NAME = "mistralai/Mistral-7B-v0.1"  # –ú–æ–¥–µ–ª—å –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è
output_dir = "/root/training/Mistral-lora-output"
epochs = 3
batch_size = 16

LORA_CONFIG = {
    "r": 8,                          # –†–∞–Ω–≥ LoRA
    "lora_alpha": 16,                # Alpha –ø–∞—Ä–∞–º–µ—Ç—Ä
    "target_modules": ["q_proj", "v_proj"],  # –¶–µ–ª–µ–≤—ã–µ —Å–ª–æ–∏
    "lora_dropout": 0.05,            # –û—Ç–∫–ª—é—á–µ–Ω–∏–µ 5% –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    "bias": "none",                  # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ bios —Å–ª–æ—ë–≤
    "task_type": "CAUSAL_LM"         # –°–ø–æ—Å–æ–± –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
}
```

### –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
```python
data_file = "data/sample_training_data.jsonl"
```

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Docker –æ–±—Ä–∞–∑–æ–≤
```python
# –í vast.ai.check.py
images_to_try = [
    "nvidia/cuda:12.1.0-base-ubuntu22.04",
    "nvidia/cuda:11.8.0-base-ubuntu22.04",
    "pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime",
]
```

## üéØ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—É—Å–∫
```bash
python vast.ai.check.py
```

### –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ:

1. **–ü–æ–∏—Å–∫ –æ—Ñ—Ñ–µ—Ä–æ–≤** - —Å–∫—Ä–∏–ø—Ç –Ω–∞—Ö–æ–¥–∏—Ç —Å–∞–º—ã–µ –¥–µ—à–µ–≤—ã–µ GPU –≤ –∑–∞–¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö
2. **–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Å—Ç–∞–Ω—Å–∞** - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞—Ä–µ–Ω–¥–∞ —Å–µ—Ä–≤–µ—Ä–∞ —Å —Å–∞–º–æ–π –¥–µ—à–æ–≤–æ–π –≤–∏–¥–µ–æ–∫–∞—Ä—Ç–æ–π –∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö
3. **–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è** - —É—Å—Ç–∞–Ω–æ–≤–∫–∞ Python, PyTorch, Transformers, PEFT
4. **–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö** - –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ —Å–µ—Ä–≤–µ—Ä
5. **Fine-tuning** - –∑–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å LoRA
6. **–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏** - –∑–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä
7. **–û—á–∏—Å—Ç–∫–∞** - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –∏–Ω—Å—Ç–∞–Ω—Å–∞

### –ü—Ä–∏–º–µ—Ä –≤—ã–≤–æ–¥–∞
```
Vast.ai Auto Instance Manager
========================================
‚úì API –¥–æ—Å—Ç—É–ø–µ–Ω –∏ —Ç–æ–∫–µ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è —á—Ç–µ–Ω–∏—è

–ü–æ–∏—Å–∫ –æ—Ñ—Ñ–µ—Ä–æ–≤ –¥–ª—è RTX 5090...
–ù–∞–π–¥–µ–Ω –æ—Ñ—Ñ–µ—Ä: RTX 5090 ‚Äî $0.0120/—á (id=12345)
‚úì –ù–∞–π–¥–µ–Ω–æ 15 –æ—Ñ—Ñ–µ—Ä–æ–≤ –¥–ª—è –ø–æ–ø—ã—Ç–∫–∏

--- –ü–æ–ø—ã—Ç–∫–∞ 1/15: RTX 5090 @ $0.012/—á ---
‚úì –ò–Ω—Å—Ç–∞–Ω—Å —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: 27921384

‚úì –ò–ù–°–¢–ê–ù–° –ì–û–¢–û–í –ö –†–ê–ë–û–¢–ï!
–ö–æ–º–∞–Ω–¥–∞ SSH: ssh root@ssh5.vast.ai -p 12982

‚úì SSH –≥–æ—Ç–æ–≤
‚úì –û–∫—Ä—É–∂–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ
‚úì –°–∫—Ä–∏–ø—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ –∏–Ω—Å—Ç–∞–Ω—Å
‚úì –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ –∏–Ω—Å—Ç–∞–Ω—Å

–ó–ê–ü–£–°–ö –î–û–û–ë–£–ß–ï–ù–ò–Ø (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—Å–æ–≤)
...
‚úì –î–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!
‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤: output/llama3-lora-model
```

## üîç –ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∫—Ä–∏–ø—Ç–∞

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. –ü–æ–∏—Å–∫ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è GPU –æ—Ñ—Ñ–µ—Ä–æ–≤          ‚îÇ
‚îÇ     - –ó–∞–ø—Ä–æ—Å –∫ Vast.ai API                  ‚îÇ
‚îÇ     - –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ü–µ–Ω–µ                    ‚îÇ
‚îÇ     - –§–∏–ª—å—Ç—Ä –ø–æ max_price                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Å—Ç–∞–Ω—Å–∞                       ‚îÇ
‚îÇ     - PUT /asks/{id} –∏–ª–∏ POST /instances    ‚îÇ
‚îÇ     - Retry –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –æ—Ñ—Ñ–µ—Ä –ø—Ä–∏ –Ω–µ—É–¥–∞—á–µ  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞                        ‚îÇ
‚îÇ     - Polling —Å—Ç–∞—Ç—É—Å–∞ –∏–Ω—Å—Ç–∞–Ω—Å–∞              ‚îÇ
‚îÇ     - –ü–æ–ª—É—á–µ–Ω–∏–µ SSH –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è (SSH)               ‚îÇ
‚îÇ     - apt-get install python3 pip git       ‚îÇ
‚îÇ     - pip install torch transformers peft   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  5. –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ (SCP)                   ‚îÇ
‚îÇ     - train.py (—Å–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è)            ‚îÇ
‚îÇ     - data.jsonl (–¥–∞—Ç–∞—Å–µ—Ç)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  6. Fine-tuning                             ‚îÇ
‚îÇ     - –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π  ‚îÇ
‚îÇ     - –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤             ‚îÇ
‚îÇ     - Training loop (3 —ç–ø–æ—Ö–∏)               ‚îÇ
‚îÇ     - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ—Å–æ–≤                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  7. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (SCP)            ‚îÇ
‚îÇ     - LoRA –≤–µ—Å–∞ (adapter_model.bin)         ‚îÇ
‚îÇ     - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (adapter_config.json)    ‚îÇ
‚îÇ     - –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  8. –û—á–∏—Å—Ç–∫–∞                                 ‚îÇ
‚îÇ     - POST /instances/{id}/stop             ‚îÇ
‚îÇ     - DELETE /instances/{id}                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏

## üìã –§—É–Ω–∫—Ü–∏–∏ vast.ai.check.py

### 1. API –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

#### `check_api_connection()`
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Vast.ai API –∏ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–∞

**–ê–ª–≥–æ—Ä–∏—Ç–º:**
```python
1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑–æ–≤–æ–≥–æ HTTP –¥–æ—Å—Ç—É–ø–∞ –∫ https://console.vast.ai
2. GET –∑–∞–ø—Ä–æ—Å –∫ /api/v0/bundles —Å limit=1
3. –í–∞–ª–∏–¥–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø–æ–ª—è "offers"
```

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:**
- `True` ‚Äî API –¥–æ—Å—Ç—É–ø–µ–Ω, —Ç–æ–∫–µ–Ω –≤–∞–ª–∏–¥–µ–Ω
- `False` ‚Äî –ü—Ä–æ–±–ª–µ–º—ã —Å –¥–æ—Å—Ç—É–ø–æ–º –∏–ª–∏ —Ç–æ–∫–µ–Ω–æ–º

---

#### `make_api_request(endpoint, method, params, json_data, retry_count=3)`
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è HTTP –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ Vast.ai API

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `endpoint` ‚Äî –ø—É—Ç—å API (–Ω–∞–ø—Ä–∏–º–µ—Ä, "bundles", "instances/123")
- `method` ‚Äî HTTP –º–µ—Ç–æ–¥ (GET, POST, PUT, DELETE)
- `params` ‚Äî query –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è GET
- `json_data` ‚Äî —Ç–µ–ª–æ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è POST/PUT
- `retry_count` ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π retry –ø—Ä–∏ —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–∫–∞—Ö
- –û–±—Ä–∞–±–æ—Ç–∫–∞ HTML –æ—Ç–≤–µ—Ç–æ–≤ –≤–º–µ—Å—Ç–æ JSON
- Timeout 30 —Å–µ–∫—É–Ω–¥
- Bearer token –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** JSON –æ–±—ä–µ–∫—Ç –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ

---

### 2. –ü–æ–∏—Å–∫ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—Ñ—Ñ–µ—Ä–∞–º–∏

#### `find_cheapest_offers(gpu_name, limit=20, max_price=None)`
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –ü–æ–∏—Å–∫ —Å–∞–º—ã—Ö –¥–µ—à–µ–≤—ã—Ö GPU –æ—Ñ—Ñ–µ—Ä–æ–≤ –Ω–∞ Vast.ai

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `gpu_name` ‚Äî –Ω–∞–∑–≤–∞–Ω–∏–µ GPU ("RTX 4090", "RTX 5090", –∏ —Ç.–¥.)
- `limit` ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ñ—Ñ–µ—Ä–æ–≤
- `max_price` ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ USD/—á–∞—Å (—Ñ–∏–ª—å—Ç—Ä)

**–ê–ª–≥–æ—Ä–∏—Ç–º:**
```python
1. GET /bundles?gpu_name={gpu_name}&order=price&direction=asc&limit={limit*2}
2. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ max_price (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
3. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–æ—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é gpu_name
4. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ü–µ–Ω–µ (ascending)
5. –í–æ–∑–≤—Ä–∞—Ç –ø–µ—Ä–≤—ã—Ö {limit} –æ—Ñ—Ñ–µ—Ä–æ–≤
```

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π:
```python
[
    {
        "id": 12345,
        "ask_contract_id": 67890,
        "bundle_id": 54321,
        "gpu_name": "RTX 5090",
        "price": 0.012
    },
    ...
]
```

---

#### `validate_bundle(bundle_id)`
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ bundle

**–ê–ª–≥–æ—Ä–∏—Ç–º:**
```python
1. GET /bundles?q=id={bundle_id}
2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ—Ñ—Ñ–µ—Ä–∞ –≤ –æ—Ç–≤–µ—Ç–µ
3. –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: gpu_name, —Ü–µ–Ω–∞, –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å, min_bid
```

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:**
- `True` ‚Äî bundle –Ω–∞–π–¥–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω
- `False` ‚Äî bundle –Ω–µ –Ω–∞–π–¥–µ–Ω

---

### 3. –°–æ–∑–¥–∞–Ω–∏–µ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Å—Ç–∞–Ω—Å–∞–º–∏

#### `create_instance(offer_ids, image, disk=10, label, runtype="ssh")`
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –°–æ–∑–¥–∞–Ω–∏–µ GPU –∏–Ω—Å—Ç–∞–Ω—Å–∞ –Ω–∞ Vast.ai

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `offer_ids` ‚Äî –∑–Ω–∞—á–µ–Ω–∏–µ id/bundle_id –∏–ª–∏ —á–∏—Å–ª–æ
- `image` ‚Äî Docker –æ–±—Ä–∞–∑ ("nvidia/cuda:12.1.0-base-ubuntu22.04")
- `disk` ‚Äî —Ä–∞–∑–º–µ—Ä –¥–∏—Å–∫–∞ –≤ GB (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 10)
- `label` ‚Äî –º–µ—Ç–∫–∞ –∏–Ω—Å—Ç–∞–Ω—Å–∞
- `runtype` ‚Äî —Ç–∏–ø –∑–∞–ø—É—Å–∫–∞ ("ssh" –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)

**–ê–ª–≥–æ—Ä–∏—Ç–º (fallback —Å—Ç—Ä–∞—Ç–µ–≥–∏—è):**
```python
1. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ payload —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∑–∞–ø—É—Å–∫–∞
2. –ü–æ–ø—ã—Ç–∫–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ endpoints:
   - PUT /asks/{ask_id}
   - PUT /asks/{bundle_id}
   - POST /instances
3. –ö–∞–∂–¥—ã–π endpoint –ø—Ä–æ–±—É–µ—Ç—Å—è —Å 2 –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ payload
4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—à–∏–±–∫—É "no_such_ask"
5. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ instance_id –∏–∑ –æ—Ç–≤–µ—Ç–∞
```

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:**
- `instance_id` (int) ‚Äî ID —Å–æ–∑–¥–∞–Ω–Ω–æ–≥–æ –∏–Ω—Å—Ç–∞–Ω—Å–∞
- `None` ‚Äî –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å

---

#### `wait_for_instance(instance_id, timeout=120, interval=10)`
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∏–Ω—Å—Ç–∞–Ω—Å–∞ –∫ SSH –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—é

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `instance_id` ‚Äî ID –∏–Ω—Å—Ç–∞–Ω—Å–∞
- `timeout` ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è (—Å–µ–∫—É–Ω–¥—ã)
- `interval` ‚Äî –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏ (—Å–µ–∫—É–Ω–¥—ã)

**–ê–ª–≥–æ—Ä–∏—Ç–º:**
```python
1. Polling GET /instances/{instance_id} –∫–∞–∂–¥—ã–µ {interval} —Å–µ–∫—É–Ω–¥
2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞: actual_status, cur_state, status
3. –î–µ—Ç–µ–∫—Ü–∏—è –æ—à–∏–±–æ–∫: "exited", "error", CDI runtime errors
4. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ SSH –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: ssh_host, ssh_port, ssh_user
5. –í—ã–≤–æ–¥ SSH –∫–æ–º–∞–Ω–¥—ã –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
```

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:**
- `True` ‚Äî –∏–Ω—Å—Ç–∞–Ω—Å –≥–æ—Ç–æ–≤, SSH –¥–æ—Å—Ç—É–ø–µ–Ω
- `False` ‚Äî –æ—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –∏–ª–∏ timeout

**–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫:**
- CDI (Container Device Interface) –æ—à–∏–±–∫–∏ ‚Üí –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞
- –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ IP –∞–¥—Ä–µ—Å–∞ ‚Üí –ø–æ–∏—Å–∫ –ø—É–±–ª–∏—á–Ω–æ–≥–æ —Ö–æ—Å—Ç–∞

---

#### `stop_and_delete(instance_id)`
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ —É–¥–∞–ª–µ–Ω–∏–µ –∏–Ω—Å—Ç–∞–Ω—Å–∞

**–ê–ª–≥–æ—Ä–∏—Ç–º:**
```python
1. POST /instances/{instance_id}/stop
2. –û–∂–∏–¥–∞–Ω–∏–µ 5 —Å–µ–∫—É–Ω–¥
3. DELETE /instances/{instance_id}
```

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** None (–≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç—É—Å –≤ –∫–æ–Ω—Å–æ–ª—å)

---

### 4. SSH –æ–ø–µ—Ä–∞—Ü–∏–∏

#### `run_ssh_command(ssh_host, ssh_user, ssh_port, command)`
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã –Ω–∞ —É–¥–∞–ª—ë–Ω–Ω–æ–º —Å–µ—Ä–≤–µ—Ä–µ —á–µ—Ä–µ–∑ SSH

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `ssh_host` ‚Äî IP –∏–ª–∏ hostname —Å–µ—Ä–≤–µ—Ä–∞
- `ssh_user` ‚Äî –∏–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–æ–±—ã—á–Ω–æ "root")
- `ssh_port` ‚Äî SSH –ø–æ—Ä—Ç (–æ–±—ã—á–Ω–æ 22)
- `command` ‚Äî –∫–æ–º–∞–Ω–¥–∞ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

**–ö–æ–º–∞–Ω–¥–∞ SSH:**
```bash
ssh -o StrictHostKeyChecking=no \
    -o UserKnownHostsFile=nul \
    -o ConnectTimeout=10 \
    -p {port} {user}@{host} {command}
```

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- –û—Ç–∫–ª—é—á–µ–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ host key (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ)
- Timeout 600 —Å–µ–∫—É–Ω–¥ (10 –º–∏–Ω—É—Ç)
- –°–ø–∏—Å–æ–∫ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å PowerShell)

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** `(stdout, stderr, returncode)`

---

#### `wait_ssh_ready(ssh_host, ssh_user, ssh_port, timeout=600, interval=5)`
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ SSH –∏–Ω—Å—Ç–∞–Ω—Å–∞

**–ê–ª–≥–æ—Ä–∏—Ç–º:**
```python
1. –ü–æ–ø—ã—Ç–∫–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç—å "echo ok" —á–µ—Ä–µ–∑ SSH
2. –ü–æ–≤—Ç–æ—Ä –∫–∞–∂–¥—ã–µ {interval} —Å–µ–∫—É–Ω–¥
3. –í—ã–≤–æ–¥ –ø—Ä–∏—á–∏–Ω—ã –æ—à–∏–±–∫–∏
```

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:**
- `True` ‚Äî SSH –≥–æ—Ç–æ–≤
- `False` ‚Äî timeout

---

### 5. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ

#### `setup_training_environment(ssh_host, ssh_user, ssh_port)`
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —É–¥–∞–ª—ë–Ω–Ω–æ–º —Å–µ—Ä–≤–µ—Ä–µ

**–ö–æ–º–∞–Ω–¥—ã:**
```bash
1. apt-get update
2. apt-get install -y python3 python3-pip git wget curl
3. pip3 install --upgrade pip
4. pip3 install torch transformers datasets peft bitsandbytes
5. pip3 install accelerate scikit-learn wandb
6. mkdir -p /root/training
```

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- –ö–∞–∂–¥–∞—è –∫–æ–º–∞–Ω–¥–∞ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è –¥–æ 3 —Ä–∞–∑ –ø—Ä–∏ –æ—à–∏–±–∫–µ
- –ó–∞–¥–µ—Ä–∂–∫–∞ 5 —Å–µ–∫—É–Ω–¥ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
- –í—ã–≤–æ–¥ —Å–æ–∫—Ä–∞—â—ë–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∫–æ–º–∞–Ω–¥—ã (–ø–µ—Ä–≤—ã–µ 50 —Å–∏–º–≤–æ–ª–æ–≤)

---

#### `upload_training_script(ssh_host, ssh_user, ssh_port, script_file="remote_train.py")`
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –ó–∞–≥—Ä—É–∑–∫–∞ —Å–∫—Ä–∏–ø—Ç–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∏–Ω—Å—Ç–∞–Ω—Å

**–ê–ª–≥–æ—Ä–∏—Ç–º:**
```python
1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
2. SCP –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ: local/remote_train.py ‚Üí remote:/root/training/train.py
```

**SCP –∫–æ–º–∞–Ω–¥–∞:**
```bash
scp -o StrictHostKeyChecking=no \
    -o UserKnownHostsFile=nul \
    -P {port} \
    remote_train.py {user}@{host}:/root/training/train.py
```

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:**
- `True` ‚Äî —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω
- `False` ‚Äî –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏

---

#### `upload_training_data(ssh_host, ssh_user, ssh_port, data_file)`
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ —Å–µ—Ä–≤–µ—Ä

**–ê–ª–≥–æ—Ä–∏—Ç–º:**
```python
1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
2. SCP –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ: local/{data_file} ‚Üí remote:/root/training/data.jsonl
```

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:**
- `True` ‚Äî –¥–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω
- `False` ‚Äî —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –æ—à–∏–±–∫–∞

---

#### `start_training(ssh_host, ssh_user, ssh_port)`
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ

**–ö–æ–º–∞–Ω–¥–∞:**
```bash
cd /root/training && python3 train.py
```

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- –°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ (–∂–¥—ë—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è)
- Timeout 600 —Å–µ–∫—É–Ω–¥
- –í—ã–≤–æ–¥ stdout –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
- –í—ã–≤–æ–¥ stderr –ø—Ä–∏ –æ—à–∏–±–∫–µ

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:**
- `True` ‚Äî –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ
- `False` ‚Äî –æ—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è

---

#### `download_trained_model(ssh_host, ssh_user, ssh_port, output_dir)`
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å —Å–µ—Ä–≤–µ—Ä–∞

**–ê–ª–≥–æ—Ä–∏—Ç–º:**
```python
1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ /root/training/ (ls -la)
2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è /root/training/Mistral-lora-output/
3. –°–æ–∑–¥–∞–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ output/Mistral-lora-model/
4. SCP —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
```

**SCP –∫–æ–º–∞–Ω–¥–∞:**
```bash
scp -o StrictHostKeyChecking=no \
    -o UserKnownHostsFile=nul \
    -P {port} -r \
    {user}@{host}:/root/training/Mistral-lora-output/* \
    {local_dir}/
```

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:**
- `local_dir` (str) ‚Äî –ø—É—Ç—å –∫ —Å–∫–∞—á–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
- `None` ‚Äî –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –æ—à–∏–±–∫–∞

---

### 6. –ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª (main)

**–ê–ª–≥–æ—Ä–∏—Ç–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:**

```python
1. –ü—Ä–æ–≤–µ—Ä–∫–∞ API —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
2. –ü–æ–∏—Å–∫ –æ—Ñ—Ñ–µ—Ä–æ–≤ –¥–ª—è –≤—Å–µ—Ö GPU –∏–∑ —Å–ø–∏—Å–∫–∞
3. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –≤—Å–µ—Ö –æ—Ñ—Ñ–µ—Ä–æ–≤ –ø–æ —Ü–µ–Ω–µ
4. –¶–∏–∫–ª –ø–æ –æ—Ñ—Ñ–µ—Ä–∞–º (–æ—Ç –¥–µ—à—ë–≤—ã—Ö –∫ –¥–æ—Ä–æ–≥–∏–º):
   a. –¶–∏–∫–ª –ø–æ Docker –æ–±—Ä–∞–∑–∞–º (fallback):
      - –ü–æ–ø—ã—Ç–∫–∞ —Å–æ–∑–¥–∞—Ç—å –∏–Ω—Å—Ç–∞–Ω—Å
      - –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞ (60 —Å–µ–∫)
      - –ü—Ä–∏ –æ—à–∏–±–∫–µ CDI ‚Üí —É–¥–∞–ª–µ–Ω–∏–µ –∏ –ø–µ—Ä–µ—Ö–æ–¥ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –æ–±—Ä–∞–∑—É
   b. –ï—Å–ª–∏ –∏–Ω—Å—Ç–∞–Ω—Å –∑–∞–ø—É—Å—Ç–∏–ª—Å—è ‚Üí –≤—ã—Ö–æ–¥ –∏–∑ —Ü–∏–∫–ª–æ–≤
5. –ü–æ–ª—É—á–µ–Ω–∏–µ SSH –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–Ω—Å—Ç–∞–Ω—Å–∞
6. –û–∂–∏–¥–∞–Ω–∏–µ SSH –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ (5 –º–∏–Ω—É—Ç)
7. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
8. –ó–∞–≥—Ä—É–∑–∫–∞ —Å–∫—Ä–∏–ø—Ç–∞ –∏ –¥–∞–Ω–Ω—ã—Ö
9. –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)
10. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
11. –¢–∞–π–º–µ—Ä –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ –∏–Ω—Å—Ç–∞–Ω—Å–∞ (30 —Å–µ–∫)
```

---

## üìã –§—É–Ω–∫—Ü–∏–∏ remote_train.py

### 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏

#### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–∫—Ä–∏–ø—Ç–∞
```python
model_name = "mistralai/Mistral-7B-v0.1"  # HuggingFace –º–æ–¥–µ–ª—å
output_dir = "/root/training/Mistral-lora-output"  # –ü—É—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
data_path = "/root/training/data.jsonl"  # –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
epochs = 3  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
batch_size = 16  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
```

---

#### BitsAndBytes –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
```python
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,              # 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (—ç–∫–æ–Ω–æ–º–∏—è VRAM)
    bnb_4bit_use_double_quant=True, # –î–≤–æ–π–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (–µ—â—ë –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏)
    bnb_4bit_quant_type="nf4",      # NormalFloat4 (–ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ)
    bnb_4bit_compute_dtype=torch.float16  # –í—ã—á–∏—Å–ª–µ–Ω–∏—è –≤ FP16
)
```

---

#### –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
```python
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",           # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ GPU
    trust_remote_code=True       # –î–æ–≤–µ—Ä—è—Ç—å –∫–æ–¥—É –º–æ–¥–µ–ª–∏
)
```

---

#### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è LoRA
```python
model = prepare_model_for_kbit_training(model)
```

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç:**
- –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç –≤–µ—Å–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
- –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ç–æ–ª—å–∫–æ –¥–ª—è LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
- –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –¥–ª—è mixed precision training

---

### 2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LoRA

```python
lora_config = LoraConfig(
    r=8,                              # –†–∞–Ω–≥ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ (—Ä–∞–∑–º–µ—Ä –∞–¥–∞–ø—Ç–µ—Ä–∞)
    lora_alpha=16,                    # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É—é—â–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç
    target_modules=["q_proj", "v_proj"],  # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ Query –∏ Value
    lora_dropout=0.05,                # Dropout 5% –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
    bias="none",                      # –ù–µ –æ–±—É—á–∞–µ–º bias —Å–ª–æ–∏
    task_type="CAUSAL_LM"             # Causal Language Modeling
)

model = get_peft_model(model, lora_config)
```

**–ß—Ç–æ —Å–æ–∑–¥–∞—ë—Ç—Å—è:**
```
–ò—Å—Ö–æ–¥–Ω–∞—è –º–æ–¥–µ–ª—å (7B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∑–∞–º–æ—Ä–æ–∂–µ–Ω–∞)
    ‚Üì
+ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã (4M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–±—É—á–∞–µ–º—ã–µ)
    ‚Üì
= –û–±—É—á–∞–µ–º–æ —Ç–æ–ª—å–∫–æ 0.06% –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤!
```

---

### 3. –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä

```python
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
```

**–ó–∞—á–µ–º –Ω—É–∂–µ–Ω pad_token:**
- –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã
- –ë–µ–∑ pad_token –±–∞—Ç—á–∏ –Ω–µ –º–æ–≥—É—Ç –±—ã—Ç—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω—ã
- –ò—Å–ø–æ–ª—å–∑—É–µ–º EOS —Ç–æ–∫–µ–Ω –∫–∞–∫ padding

---

### 4. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö

#### –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
```python
dataset = load_dataset("json", data_files=data_path)
```

**–§–æ—Ä–º–∞—Ç JSONL:**
```jsonl
{"text": "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è 1"}
{"text": "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è 2"}
{"text": "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è 3"}
```

---

#### –§—É–Ω–∫—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
```python
def formatting_func(example):
    text = example.get("text", "") or example.get("content", "")
    return {"text": text}
```

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:**
- –£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ–ª—è (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ "text" –∏ "content")
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–∞

---

#### –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
```python
def tokenize_func(examples):
    result = tokenizer(
        examples["text"],
        padding="max_length",
        max_length=512,
        truncation=True
    )
    result["labels"] = result["input_ids"].copy()
    return result

tokenized_dataset = dataset.map(tokenize_func, batched=True, remove_columns=["text"])
```

**–®–∞–≥–∏:**
1. –¢–µ–∫—Å—Ç ‚Üí —Ç–æ–∫–µ–Ω—ã (—á–∏—Å–ª–∞)
2. Padding –¥–æ 512 —Ç–æ–∫–µ–Ω–æ–≤ (–º–∞–∫—Å–∏–º—É–º 512 —Ç–æ–∫–µ–Ω–æ–≤ —Ç–µ–∫—Å—Ç–∞)
3. Truncation –µ—Å–ª–∏ –±–æ–ª—å—à–µ 512 (–±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ —á–∞—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞)
4. –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ input_ids –≤ labels (–¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è loss(—É—Ä–æ–≤–µ–Ω—å –æ—à–∏–±–æ–∫ –º–æ–¥–µ–ª–∏))

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
```python
{
    "input_ids": [1, 234, 5678, ..., 2],  # –¢–æ–∫–µ–Ω—ã –≤—Ö–æ–¥–∞
    "attention_mask": [1, 1, 1, ..., 0],  # –ú–∞—Å–∫–∞ (1=—Ä–µ–∞–ª—å–Ω—ã–π, 0=padding)
    "labels": [1, 234, 5678, ..., 2]      # –¢–æ–∫–µ–Ω—ã –¥–ª—è loss
}
```

---

### 5. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è (TrainingArguments)

```python
training_args = TrainingArguments(
    output_dir=output_dir,                    # –ü—É—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    overwrite_output_dir=True,                # –ü–µ—Ä–µ–∑–∞–ø–∏—Å—å
    num_train_epochs=epochs,                  # 3 —ç–ø–æ—Ö–∏
    per_device_train_batch_size=batch_size,   # 16 –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ GPU
    save_steps=50,                            # Checkpoint –∫–∞–∂–¥—ã–µ 50 —à–∞–≥–æ–≤
    save_total_limit=3,                       # –•—Ä–∞–Ω–∏—Ç—å 3 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö
    logging_steps=10,                         # –õ–æ–≥–∏ –∫–∞–∂–¥—ã–µ 10 —à–∞–≥–æ–≤
    learning_rate=2e-4,                       # LR = 0.0002 (–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏)
    weight_decay=0.001,                       # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (–®—Ç—Ä–∞—Ñ –∑–∞ –±–æ–ª—å—à–∏–µ –≤–µ—Å–∞)
    warmup_steps=10,                          # –ü—Ä–æ–≥—Ä–µ–≤ –ø–µ—Ä–≤—ã–µ 10 —à–∞–≥–æ–≤
    gradient_accumulation_steps=2,            # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –±–∞—Ç—á = 32
    fp16=True,                                # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–∞ (—Ä–∞—Å—á—ë—Ç—ã –≤ 16 –±–∏—Ç)
    gradient_checkpointing=True,              # –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏
    report_to="none",                         # –û—Ç–∫–ª—é—á–∏—Ç—å WandB
)
```

---

### 6. Trainer –∏ –æ–±—É—á–µ–Ω–∏–µ

```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    tokenizer=tokenizer
)

trainer.train()
```

**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä–∏:**
```python
for epoch in range(3):
    for batch in dataloader:
        # Forward pass
        outputs = model(batch["input_ids"], labels=batch["labels"])
        loss = outputs.loss
        
        # Backward pass
        loss.backward()
        
        # Gradient accumulation
        if step % gradient_accumulation_steps == 0:
            optimizer.step()  # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
            optimizer.zero_grad()  # –û—á–∏—Å—Ç–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        
        # Logging
        if step % logging_steps == 0:
            print(f"Step {step}, Loss: {loss:.4f}")
        
        # Checkpointing
        if step % save_steps == 0:
            model.save_pretrained(f"{output_dir}/checkpoint-{step}")
```

---

### 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

```python
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
```

**–ß—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è:**

```
/root/training/Mistral-lora-output/
‚îú‚îÄ‚îÄ adapter_model.safetensors   # LoRA –≤–µ—Å–∞ (~50 MB)
‚îú‚îÄ‚îÄ adapter_config.json         # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LoRA
‚îú‚îÄ‚îÄ tokenizer.json              # –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä
‚îú‚îÄ‚îÄ tokenizer_config.json       # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
‚îú‚îÄ‚îÄ special_tokens_map.json     # –°–ø–µ—Ü—Ç–æ–∫–µ–Ω—ã
‚îî‚îÄ‚îÄ trainer_state.json          # –õ–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è
```

**–í–∞–∂–Ω–æ:** –°–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã, –Ω–µ –≤—Å—è –º–æ–¥–µ–ª—å!

---

### 8. –í—ã–≤–æ–¥ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π

```python
print(f"Training completed! Model saved to {output_dir}")
print(f"To use this model:")
print(f"  from peft import AutoPeftModelForCausalLM")
print(f"  model = AutoPeftModelForCausalLM.from_pretrained('{output_dir}')")
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**
```python
from peft import AutoPeftModelForCausalLM

# –ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å + LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã
model = AutoPeftModelForCausalLM.from_pretrained(
    "/root/training/Mistral-lora-output"
)
```

---

## üîÑ –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è

```
1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Mistral-7B-v0.1 (4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è)
   ‚Üì
2. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ (r=8, —Ç–æ–ª—å–∫–æ 0.06% –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
   ‚Üì
3. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ JSONL
   ‚Üì
4. –û–±—É—á–µ–Ω–∏–µ (3 —ç–ø–æ—Ö–∏, batch=16, accumulation=2)
   - –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –±–∞—Ç—á = 32
   - FP16 mixed precision
   - Gradient checkpointing
   - AdamW –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (lr=2e-4)
   ‚Üì
5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ (~50 MB)
   ‚Üì
6. –í—ã–≤–æ–¥ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
```

---


### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LoRA
| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–∏–µ | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|----------|----------|
| `r` | 8 | –†–∞–Ω–≥ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ (—Ä–∞–∑–º–µ—Ä –∞–¥–∞–ø—Ç–µ—Ä–∞) |
| `lora_alpha` | 16 | –ú–∞—Å—à—Ç–∞–±–∏—Ä—É—é—â–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç |
| `target_modules` | `["q_proj", "v_proj"]` | –°–ª–æ–∏ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ |
| `lora_dropout` | 0.05 | Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ |

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã Training
| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–∏–µ | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|----------|----------|
| `num_train_epochs` | 3 | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö |
| `per_device_train_batch_size` | 16 | –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ |
| `gradient_accumulation_steps` | 2 | –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –±–∞—Ç—á = 8 |
| `learning_rate` | 2e-4 | –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è |
| `fp16` | True | Mixed precision |
| `gradient_checkpointing` | True | –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ |
| `report_to` | "none" | –û—Ç–∫–ª—é—á–µ–Ω WandB |

### –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ VRAM
| –ú–æ–¥–µ–ª—å | VRAM (4-bit) | VRAM (8-bit) | VRAM (FP16) |
|--------|--------------|--------------|-------------|
| Mistral-7B | ~6 GB | ~10 GB | ~16 GB |
| Llama-2-13B | ~10 GB | ~16 GB | ~28 GB |
| Llama-3-8B | ~7 GB | ~12 GB | ~18 GB |

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
modelup/
‚îú‚îÄ‚îÄ vast.ai.check.py           # –û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
‚îú‚îÄ‚îÄ remote_train.py            # –°–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è (–∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –Ω–∞ —Å–µ—Ä–≤–µ—Ä)
‚îú‚îÄ‚îÄ VAST_AI_AUTOMATION.md      # –≠—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
‚îú‚îÄ‚îÄ Readme.md                  # –û–±—â–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ sample_training_data.jsonl  # –î–∞–Ω–Ω—ã–µ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è
‚îî‚îÄ‚îÄ output/
    ‚îî‚îÄ‚îÄ Mistral-lora-model/    # –°–∫–∞—á–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (—Å–æ–∑–¥–∞—ë—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        ‚îú‚îÄ‚îÄ adapter_model.safetensors
        ‚îú‚îÄ‚îÄ adapter_config.json
        ‚îú‚îÄ‚îÄ tokenizer.json
        ‚îî‚îÄ‚îÄ –¥—Ä—É–≥–∏–µ —Ñ–∞–π–ª—ã —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞...
```

## üîß Troubleshooting

### –ü—Ä–æ–±–ª–µ–º–∞: "401 Unauthorized" –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ API
**–†–µ—à–µ–Ω–∏–µ**: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ BEARER_TOKEN –≤ —Å–∫—Ä–∏–ø—Ç–µ. –ü–æ–ª—É—á–∏—Ç–µ –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω –≤ [Account Settings](https://cloud.vast.ai/account/).

### –ü—Ä–æ–±–ª–µ–º–∞: "Permission denied (publickey)" –ø—Ä–∏ SSH
**–†–µ—à–µ–Ω–∏–µ**: 
1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ SSH –∫–ª—é—á –¥–æ–±–∞–≤–ª–µ–Ω –≤ Vast.ai –∞–∫–∫–∞—É–Ω—Ç
2. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø—Ä–∏–≤–∞—Ç–Ω—ã–π –∫–ª—é—á –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ `~/.ssh/id_ed25519` –∏–ª–∏ `~/.ssh/id_rsa`

### –ü—Ä–æ–±–ª–µ–º–∞: "No such file or directory" –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏
**–†–µ—à–µ–Ω–∏–µ**: –û–±—É—á–µ–Ω–∏–µ –Ω–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å –∏–ª–∏ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å —Å –æ—à–∏–±–∫–æ–π. –°–∫—Ä–∏–ø—Ç —Ç–µ–ø–µ—Ä—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç:
```
–°–æ–¥–µ—Ä–∂–∏–º–æ–µ /root/training/:
total 12
drwxr-xr-x 2 root root 4096 Nov 18 19:45 .
drwx------ 1 root root 4096 Nov 18 19:30 ..
-rw-r--r-- 1 root root 1234 Nov 18 19:35 data.jsonl
-rw-r--r-- 1 root root 5678 Nov 18 19:35 train.py
```
–ï—Å–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ `Mistral-lora-output` –Ω–µ—Ç, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è –≤ —Å–µ–∫—Ü–∏–∏ "–ó–ê–ü–£–°–ö –î–û–û–ë–£–ß–ï–ù–ò–Ø" –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –æ—à–∏–±–∫–∏.

### –ü—Ä–æ–±–ª–µ–º–∞: "GatedRepoError: Cannot access gated repo"
**–†–µ—à–µ–Ω–∏–µ**: 
- –ú–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–∏–Ω—è—Ç–∏—è –ª–∏—Ü–µ–Ω–∑–∏–∏ –Ω–∞ HuggingFace (–Ω–∞–ø—Ä–∏–º–µ—Ä, Llama-2, Llama-3)
- –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –º–æ–¥–µ–ª–∏ –Ω–∞ HuggingFace –∏ –ø—Ä–∏–º–∏—Ç–µ —É—Å–ª–æ–≤–∏—è
- **–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è**: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ—Ç–∫—Ä—ã—Ç—É—é –º–æ–¥–µ–ª—å `mistralai/Mistral-7B-v0.1` (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤ `remote_train.py`)
- –ò–ª–∏ –∏–∑–º–µ–Ω–∏—Ç–µ `model_name` –≤ `remote_train.py` –Ω–∞ –ª—é–±—É—é –æ—Ç–∫—Ä—ã—Ç—É—é –º–æ–¥–µ–ª—å

### –ü—Ä–æ–±–ª–µ–º–∞: CUDA Out of Memory
**–†–µ—à–µ–Ω–∏–µ**:
- –£–º–µ–Ω—å—à–∏—Ç–µ `batch_size` –≤ train.py (—Å 16 –¥–æ 4 –∏–ª–∏ 2)
- –£–º–µ–Ω—å—à–∏—Ç–µ `max_length` —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (—Å 512 –¥–æ 256)
- –í—ã–±–µ—Ä–∏—Ç–µ GPU —Å –±–æ–ª—å—à–∏–º VRAM

### –ü—Ä–æ–±–ª–µ–º–∞: "No offers found"
**–†–µ—à–µ–Ω–∏–µ**:
- –£–≤–µ–ª–∏—á—å—Ç–µ `max_price` (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–æ 0.6)
- –î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ GPU –≤ `gpu_list`
- –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å –≤ –¥—Ä—É–≥–æ–µ –≤—Ä–µ–º—è —Å—É—Ç–æ–∫

### –ü—Ä–æ–±–ª–µ–º–∞: Training stuck –∏–ª–∏ –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω—ã–π
**–†–µ—à–µ–Ω–∏–µ**:
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU: `nvidia-smi` —á–µ—Ä–µ–∑ SSH
- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ model –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ GPU (device_map="auto")
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ - —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π (< 100 –ø—Ä–∏–º–µ—Ä–æ–≤) –º–æ–∂–µ—Ç –±—ã—Ç—å –±—ã—Å—Ç—Ä—ã–º


## üìù –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏

```python
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = AutoPeftModelForCausalLM.from_pretrained(
    "output/Mistral-lora-model",
    device_map="auto",
    torch_dtype="auto"
)

tokenizer = AutoTokenizer.from_pretrained("output/Mistral-lora-model")

# –ò–Ω—Ñ–µ—Ä–µ–Ω—Å
prompt = "–í–æ–ø—Ä–æ—Å: –ß—Ç–æ —Ç–∞–∫–æ–µ LoRA?\n–û—Ç–≤–µ—Ç:"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

## üîß –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è

–í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Ñ–∞–π–ª–µ `remote_train.py`:

```python
# –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
model_name = "mistralai/Mistral-7B-v0.1"  # –ò–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å
output_dir = "/root/training/Mistral-lora-output"
data_path = "/root/training/data.jsonl"
epochs = 3  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
batch_size = 16  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞

# LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
lora_config = LoraConfig(
    r=8,  # –£–≤–µ–ª–∏—á—å—Ç–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ (16, 32)
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],  # –î–æ–±–∞–≤—å—Ç–µ "k_proj", "o_proj" –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –æ—Ö–≤–∞—Ç–∞
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Training –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
training_args = TrainingArguments(
    learning_rate=2e-4,  # –£–º–µ–Ω—å—à–∏—Ç–µ –¥–ª—è –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    per_device_train_batch_size=batch_size,
    gradient_accumulation_steps=2,  # –£–≤–µ–ª–∏—á—å—Ç–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –±–∞—Ç—á–µ–π
    num_train_epochs=epochs,
    # ... –¥—Ä—É–≥–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
)
```

## ü§ù Contributing

–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é—Ç—Å—è pull requests! –î–ª—è —Å–µ—Ä—å—ë–∑–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å–Ω–∞—á–∞–ª–∞ –æ—Ç–∫—Ä–æ–π—Ç–µ issue.


## üîó –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- [Vast.ai Documentation](https://vast.ai/docs/)
- [PEFT Documentation](https://huggingface.co/docs/peft)
- [Transformers Documentation](https://huggingface.co/docs/transformers)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes)

## üìß –ö–æ–Ω—Ç–∞–∫—Ç—ã

–ü–æ –≤–æ–ø—Ä–æ—Å–∞–º –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º —Å–æ–∑–¥–∞–≤–∞–π—Ç–µ issues –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏.

---

**‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Vast.ai —Ç—Ä–µ–±—É–µ—Ç –æ–ø–ª–∞—Ç—ã. –í—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –±–∞–ª–∞–Ω—Å –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–π—Ç–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é —Ü–µ–Ω—É. –°–∫—Ä–∏–ø—Ç –Ω–µ –Ω–µ—Å—ë—Ç –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –∑–∞ –≤–∞—à–∏ —Ä–∞—Å—Ö–æ–¥—ã.
